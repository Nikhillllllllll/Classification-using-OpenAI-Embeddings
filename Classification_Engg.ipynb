{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba185e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = \"E:\\\\BootcampGenAI\\\\repository\\\\Learn-AI-Engineering-main\\\\Learn-AI-Engineering-main\\\\learn_ai\"\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcde20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY_FUTUREPATH_ML\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf3e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "# Set the OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY_FUTUREPATH_ML')\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(5))\n",
    "def chat_completion_request(messages,                            \n",
    "                            temperature=0.5,\n",
    "                            seed=123,\n",
    "                            model=None,\n",
    "                            top_p=1,\n",
    "                            max_tokens=None,\n",
    "                            response_format=None,\n",
    "                            functions=None,\n",
    "                            function_call=None):\n",
    "    if model is None:\n",
    "        model = 'gpt-3.5-turbo-0125'\n",
    "    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            seed=seed,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            functions=functions,\n",
    "            function_call=function_call\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception: {e}\")\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(5))\n",
    "def chat_completion_request_tokens(messages,                            \n",
    "                            temperature=0,\n",
    "                            seed=123,\n",
    "                            model=None,\n",
    "                            top_p=1,\n",
    "                            max_tokens=None,\n",
    "                            functions=None,\n",
    "                            function_call=None):\n",
    "    if model is None:\n",
    "        model = 'gpt-3.5-turbo-0125'\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            seed=seed,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            functions=functions,\n",
    "            function_call=function_call\n",
    "        )\n",
    "\n",
    "        # Extract token usage from the completion response\n",
    "        total_tokens = completion.usage.total_tokens\n",
    "        input_tokens = completion.usage.prompt_tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        return completion, (total_tokens, input_tokens, output_tokens)\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to generate ChatCompletion response\")\n",
    "        logging.error(f\"Exception: {e}\")\n",
    "        return None, (0, 0, 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63693ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"E:/BootcampGenAI/repository/Learn-AI-Engineering-main/Learn-AI-Engineering-main/learn_ai/notebooks/Assignments/1.Classification/cohort-1-text-classification-challenge/train.csv\")\n",
    "\n",
    "test_df = pd.read_csv(\"E:/BootcampGenAI/repository/Learn-AI-Engineering-main/Learn-AI-Engineering-main/learn_ai/notebooks/Assignments/1.Classification/cohort-1-text-classification-challenge/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "model = OpenAIEmbeddings(model = 'text-embeddings-3-small')\n",
    "texts = train_df['text'].tolist()\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts: List[str], model: str = EMBEDDING_MODEL) -> List[str]:\n",
    "    \"\"\"Return the embeddings for a list of texts.\"\"\"\n",
    "    return client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "response = create_embeddings(texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
